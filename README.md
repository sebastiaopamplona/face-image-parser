Descreveres os dados e pré-processamento, incluindo como dividiste em treino e , se fizeste data augmentation (e como) e a geração dos batches para treino:
  - **dados**:
  IMDB-WIKI (https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/), dataset com 500K+ imagens de rosto, construído com *web scraping* dos sites IMDB e Wikipedia, com anotações da idade; dividido em 2 datasets:
    - WIKI: 62,328 imagens de rosto
    - IMDB: 460,723 imagens de rosto
  - **como dividiste em treino**:
  - **se fizeste data augmentation (e como)**:
  - **geração dos batches para treino**:
